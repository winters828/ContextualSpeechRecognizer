{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Data download and reorganisation**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys\nimport matplotlib\nimport os\nfrom nltk.corpus import stopwords #Stopwords of course\ntrain_df = pd.read_csv('/kaggle/input/traincsv/train.csv').fillna(' ')\ntrain_df.sample(10, random_state=13)\n#reorganisation of dataset: 6 columns --> 1 column\ntrain_df = train_df.assign(sum =0)\ntrain_df.loc[(train_df['toxic'] == 1) |(train_df['severe_toxic'] == 1 )|(train_df['obscene'] == 1) |\n             (train_df['threat'] == 1) |(train_df['insult'] == 1 )|(train_df['identity_hate'] == 1) , 'sum'] = 1\ntrain_df.drop([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"], axis = 1, inplace = True)\ntrain_df.info()\ntrain_df.describe()\ntrain_df.head(100)\n\nstop_words = set(stopwords.words(\"english\"))\nstop_words.add(\"!!!\")\nprint(stop_words)\n\n# Notes -------------------------------------------------------------------------------------\n# A really good Pandas function resource\n# https://www.analyticsvidhya.com/blog/2021/05/pandas-functions-13-most-important/\n\n# Just a tutorial on NN, doesn't seem to be useful\n# https://www.youtube.com/watch?v=tMrbN67U9d4 left off at 3:17","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.loc[train_df['sum']==1].sample(10, random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = train_df[train_df['sum'] == 1 ][0:2000] #here you can set up the size of training data (affects the time of training)\ndf3 = train_df[train_df['sum'] == 0 ][0:2000] #here you can set up the size of training data (affects the time of training)\n#this doesn't give the best accuracy and done for speed only. set up it to 20000 and 20000 to have accuracy =~ 90%\nframes = [df2, df3]\nresult = pd.concat(frames)\nresult['comment_text'] = result['comment_text'].replace({'@':''}, regex=True)\nresult['comment_text'] = result['comment_text'].replace({'\\n':''}, regex=True)\nresult\nresult['sum'].plot(kind='hist', title='test');\nresult['comment_text'] = result['comment_text'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = result['sum'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tokenization and Stemming**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import sent_tokenize #sent=sentence\nfrom nltk.tokenize import PunktSentenceTokenizer, word_tokenize #For tagging words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport re\n  \nps = PorterStemmer()\n\ncomments = result['comment_text'].values\n\ndef to_tokens(text):\n    org_comments = []\n    for i in text:\n        d = word_tokenize(i)\n        org_comments.append(d)\n        for w in d:\n            if w in stop_words:\n                d.remove(w)\n            if ((bool(re.match('^(?=.*[a-zA-Z])', w)))==False): #delete punctuation with regular expressions\n                d.remove(w)\n            else:\n                w = ps.stem(w)\n    return org_comments\n\norg_comments = to_tokens(comments)\norg_comments2 = deepcopy(org_comments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Token work and Pad Text Data**","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom nltk.stem import PorterStemmer as ps\nfrom nltk.tokenize import word_tokenize\nfrom tokenizers import Tokenizer\nfrom copy import deepcopy\n\n# Functions \ndef process_content():  #Speech tagging function\n    try:\n        for i in unfiltered_sent1:\n            tagged = nltk.pos_tag(unfiltered_sent1)\n        print(tagged)\n    except Exception as e:\n        print(str(e))\n\ndef flatten(input):\n    new_list = []\n    for i in input:\n        for j in i:\n            new_list.append(j)\n    return new_list\n\ndef idGenerator(array):\n    set_array = set(array) #set() removes any repeating words\n    #Sets can't be subscripted, therefor, we need to move it to a new subscriptable list\n    word_list = list(set_array)\n    id_list = [0]*len(word_list)\n    for i in range(len(word_list)):\n        id_list[i] = i; #i changes i+1 to i, otherwise training dimensions problems\n    zip_iterator = zip(word_list, id_list)\n    return dict(zip_iterator)\n        \ndef str_to_int(org_comments): \n    AllWords = flatten(org_comments)\n    dict1 = idGenerator(AllWords)\n    #replacing strings with  integers\n    for i in org_comments:\n        a = 0\n        for j in i:\n            i[a] = dict1.get(j)\n            a+=1\n    return org_comments\n\norg_comments = str_to_int(org_comments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#what is the longest comment?\narr_of_length = []\nfor i in org_comments:\n    arr_of_length.append(len(i))\nmax_number = max(arr_of_length) #the max length of comments\nx_train_val = sequence.pad_sequences(org_comments, maxlen=max_number+1) #x_train_values are ready","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Embedding Matrix and Embedding Layer**","metadata":{}},{"cell_type":"markdown","source":"We can greatly improve embeddings by learning them using a neural network on a supervised task. The embeddings form the parameters — weights — of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories — relative to the task — are closer to one another.\n\nDistributed word representation approaches such as Word2vec usually learn word vectors from a large corpus based on the distributional hypothesis.","metadata":{}},{"cell_type":"markdown","source":"**Word2Vec try**","metadata":{}},{"cell_type":"code","source":"from gensim.models import word2vec\n# Set values for various parameters\n#print(org_comments2)\nnum_features = 200    # Word vector dimensionality                      \nmin_word_count = 1   # Minimum word count, have to set up this =1 otherwise does not work                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n# Initialize and train the model (this will take some time)\nmodelWv = word2vec.Word2Vec(org_comments2, workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n#modelWv.init_sims(replace=True) # marks the end of training to speed up the use of the model\n#the model is created and trained","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = list(modelWv.wv.index_to_key)\n#print(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(modelWv.wv['Nope']) #some vector for some word in the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Embedding\nweights = modelWv.wv.vectors\n\nembedding_layer = Embedding(\n    input_dim=weights.shape[0], output_dim=weights.shape[1],\n    weights=[weights], trainable=True #i changed it to True to experiment with the NN, but it must be False\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's see what vectors look like\nmodelWv.wv.vectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Word2Vec 2d illustration**","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot\n\n# train model\nmodel2 = Word2Vec(org_comments2, min_count=10, vector_size=2)\n\n# fit a 2d PCA model to the vectors\nX = model2.wv.vectors\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\n# create a scatter plot of the projection\nfig = pyplot.figure(figsize=(10,10))\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(model2.wv.key_to_index)\nt = 0\nfor i, word in enumerate(words):\n    if (t%1==0):\n        pyplot.annotate(word, xy=(result[i,0], result[i,1]))\n    t = t + 1\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Combining layers into the Keras model**","metadata":{}},{"cell_type":"code","source":"filters = 250\nkernel_size = 3\nhidden_dims = 250","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#keras model, adding its layers\nkeras_model = Sequential()\nkeras_model.add(embedding_layer)\nkeras_model.add(Dropout(0.2))\nkeras_model.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu'))\nkeras_model.add(MaxPooling1D())\nkeras_model.add(Conv1D(filters,\n                 5,\n                 padding='valid',\n                 activation='relu'))\nkeras_model.add(GlobalMaxPooling1D())\nkeras_model.add(Dense(hidden_dims, activation='relu'))\nkeras_model.add(Dropout(0.2))\nkeras_model.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training the Keras model**","metadata":{}},{"cell_type":"code","source":"#training\nx_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.15, random_state=1)\nbatch_size = 32\nepochs = 3\n\nkeras_model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_val, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model evaluation**","metadata":{}},{"cell_type":"code","source":"keras_model.evaluate(x_val, y_val, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Our custom tests; result visualisation**","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nimport re\nps = PorterStemmer()\ntester2 = [\"What do you think about this idea?\", \"Let's agree to disagree\", \"Just shut up, you opinion is stupid\", \"Eat a plate of shit\", \"Hello people have a good day\", \"Let's play the guitar\", \"Stop this crap this is crazy\", \"My dog is cleverer than you\"]\ntester2 = pd.DataFrame (tester2, columns = ['Comments'])\ntester2['Comments'] = tester2['Comments'].replace({'@':''}, regex=True)\ntester2['Comments'] = tester2['Comments'].replace({'\\n':''}, regex=True)\ntester2['Comments'] = tester2['Comments'].str.lower()\ntester = tester2['Comments'].values\norg_tester = to_tokens(tester)\norg_tester3 = str_to_int(org_tester)\nx_testing = sequence.pad_sequences(org_tester3, maxlen=max_number+1)\ny_testing = keras_model.predict(x_testing, verbose = 1, batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tester2['Toxic'] = ['not toxic' if x < 0.50 else 'toxic' for x in y_testing]\ntester2[['Comments', 'Toxic']].head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#probabilities for all results\ny_testing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **How to save and then download a trained model**","metadata":{}},{"cell_type":"code","source":"keras_model.save('/kaggle/working/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#after saving the model it is possible to load it here\nfrom tensorflow import keras\nkeras_model = keras.models.load_model('/kaggle/working/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}